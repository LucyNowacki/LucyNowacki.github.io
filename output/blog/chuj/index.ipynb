{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "041c5ba8",
   "metadata": {},
   "source": [
    "Write your post here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8a3ef5",
   "metadata": {},
   "source": [
    "## SVD and Dimensionality Reduction\n",
    "Given $X\\in \\mathbb{R}^{s\\times n}$, consider the low rank approximation problem\n",
    "$$ \\underset{R}{\\min }\\, \\| X-R\\| _{\\text{Fro}}^2\\,\\,\\,subject\\,\\,to\\,\\, rank(R)=k$$\n",
    "where $\\| A\\| _{\\text{Fro}}\\text{:=}\\,\\sum _{i=1}^s \\sum _{j=1}^n a _{\\text{ij}}^2$, the entrywise squared $\\ell_2$ norm.\n",
    "\n",
    "To understand the PCA, it is good to delve into SVD matrix decomposition, the kernel of data compression, recommender systems, linear autoencoders and umpteen other machine-learning applications. SVD is the generalisation of Fourier transformation to more complex systems. In other words, SVD is a matrix factorisation that generalises the concept of eigendecopmpositions of square matrices. Namely, it can be shown that every real (and complex) matrix $X\\in \\mathbb{R^{s\\times n}}$ can be factorised into three matrices $U\\in \\mathbb{R^{s\\times s}}$, $\\Sigma \\in \\mathbb{R^{s \\times n}}$ and $V \\in \\mathbb{R^{n \\times n}}$  through\n",
    "\n",
    "$$X = U \\Sigma V^*\\,\\,\\,\\,(2.10)$$ \n",
    "\n",
    "\n",
    "Here both $U$ and $V$ are unitary matrices of eigenvectors whose columns are called left and right singular vectors of $X$. \n",
    "For the real matrices $X = U \\Sigma V^T=\\sigma_1U_1V^T_1+\\sigma_2 U_2 V^T_1 +...+\\sigma_n U_n V^T_n$. The \n",
    "The matrix $\\Sigma$ is a diagonal matrix of decreasing eigenvalues of the form:\n",
    "- for $s>n$\n",
    "$$\\left(\n",
    "\\begin{array}{cccc}\n",
    " \\sigma _1 & 0 & \\ldots  & 0 \\\\\n",
    " 0 & \\sigma _2 & \\ldots  & 0 \\\\\n",
    " 0 & 0 & \\ddots & 0 \\\\\n",
    " 0 & 0 & \\ldots  & \\sigma _n \\\\\n",
    " 0 & 0 & \\ldots  & 0 \\\\\n",
    " \\vdots  & \\vdots  & \\ddots & 0 \\\\\n",
    " 0 & 0 & \\ldots  & 0 \\\\\n",
    "\\end{array}\n",
    "\\right)$$\n",
    "\n",
    "- for $s<n$\n",
    "\n",
    "$$\\left(\n",
    "\\begin{array}{ccccccc}\n",
    " \\sigma _1 & 0 & \\cdots  & 0 & 0 & \\cdots  & 0 \\\\\n",
    " 0 & \\sigma _2 & \\cdots  & 0 & 0 & \\cdots  & 0 \\\\\n",
    " 0 & 0 & \\ddots & 0 & 0 & \\cdots  & 0 \\\\\n",
    " 0 & 0 & \\ldots  & \\sigma _s & 0 & \\ldots  & 0 \\\\\n",
    "\\end{array}\n",
    "\\right)$$\n",
    "SVD means that every matrix can be expressed  by rotation -> scaling -> another rotation that is illustrated below\n",
    "![img info](/images/svd.png)\n",
    "** Make Wiki dontation!!! **\n",
    "\n",
    "- for $s=n$ there is a symmetric singular matrix.\n",
    "The entrie  $\\{\\sigma_j\\}_{j=1}^{min (s,n)}$ are called singular values of $X$, and are all-nonnegative, i.e. ${\\sigma_j \\geq 0 \\,\\,\\forall j\\in\\{1,...,min(s,n)\\}}$.\n",
    "Notice that SVD of $X$ allows us to easily compute the Frobienius norm, $\\| \\Sigma \\| _{\\text{Fro}}^2=\\sqrt{\\sum _{i=1}^s \\sum _{j=1}^n \\left| x_{\\text{ij}}\\right| {}^2}$, of that matrix since the Frobenius norm is equivalent to the Euclidian (EU) norm of the vector of singular values, which we see from \n",
    "$$\\| \\Sigma \\| _{\\text{Fro}}^2=\\langle X,X\\rangle =\\left\\langle \\text{U$\\Sigma $V}^T,\\text{U$\\Sigma $V}^T\\right\\rangle =\\left\\langle \\text{$\\Sigma $V}^T,U^T \\text{U$\\Sigma $V}^T\\right\\rangle =\\left\\langle \\Sigma ,\\text{V$\\Sigma $} V^T\\right\\rangle =\\langle \\Sigma ,\\Sigma \\rangle =\\| \\Sigma \\| _{\\text{Fro}}^2=\\sum _{j=1}^{\\min (S,n)} \\sigma _j^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dee1aa1",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "env": {},
   "interrupt_mode": "signal",
   "language": "python",
   "metadata": {
    "debugger": true
   },
   "name": "python3"
  },
  "nikola": {
   "category": "",
   "date": "2023-07-24 12:16:34 UTC+01:00",
   "description": "",
   "link": "",
   "slug": "chuj",
   "tags": "Machine Learning",
   "title": "chuj",
   "type": "text"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
