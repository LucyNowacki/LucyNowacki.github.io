{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a7929460",
   "metadata": {},
   "source": [
    "The problem of finding a low-rank matrix approximation $\\hat{L}\\in \\mathbb{R}^{n\\times s}$ to a (centered) matrix $X\\in \\mathbb{R}^{n\\times s}$ (of potentially full rank) can be formulated as the optimiation problem\n",
    "\n",
    "$$\\hat{L}=\\underset{L\\in \\mathbb{R}^{n\\times s}}{\\arg  \\min } \\left\\{\\frac{1}{2} \\| L-X\\| _{\\text{Fro}}^2+\\alpha  \\| L\\| _*\\right\\} \\,\\,\\,\\,\\,(1) $$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "11e4dff9",
   "metadata": {},
   "source": [
    "Here, $\\| \\cdot \\| _*$ is so-called nuclear norm, which is the one-norm vector of singular values of $X$, which is simply the sum of singular values, i.e.\n",
    "\n",
    "$$\\| L\\| _*=\\sum _{j=1}^{\\min (n,s)} \\sigma _j$$\n",
    "\n",
    "where $\\left\\{\\sigma _j\\right\\}_{j=1}^{\\min (n,s)}$ denotes the singular values of $L$, and $\\alpha > 0$ is the regularistion parameter ( to control the sparsity of singular values of $L$ ). \n",
    "Thus, if we want to minimise (1), the $\\| \\cdot \\| _*$ produces sparse solution in terms of the singular values, namely the obtained matrix with many singular values cut to zeros, and hence the rank of $L$ is automatically lower than rank of $X$.\n",
    "Problem (1) will find a matrix $\\hat{L}$ close to X, with lower rank, depending on the choice of $\\alpha$. If $X$, however, is only low-rank up to some sparsely distributed but significant outliers, model (1) is of no good use for the approximation of this low-rank matrix. We therefore replace the squared Frobenius-norm in (1) with the matrix one-norm, i.e. $\\| \\cdot \\| _1\\text{:=}\\sum _{j=1}^n \\sum _{i=1}^s \\left| \\cdot _{\\text{ij}}\\right|$ , and obtain\n",
    "\n",
    "$$\\hat{L}=\\underset{L\\in \\mathbb{R}^{n\\times s}}{\\arg  \\min }\\left\\{\\alpha  \\| L\\| _*+\\| L-X\\| _1\\right\\}\\,\\,\\,\\,\\,(2) $$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f5f20cae",
   "metadata": {},
   "source": [
    "If we substitute $S=L-X$ in (2), we have\n",
    "\n",
    "$$\\left(\\hat{L},\\hat{S}\\right)=\\,\\,\\, \\underset{L\\in \\mathbb{R}^{n\\times s},S\\in \\mathbb{R}^{n\\times s}}{\\arg  \\min } \\left\\{\\| S\\| _1+\\alpha \\| L\\|_*\\,\\,  \\text{subject}\\,\\, \\text{to}\\,\\, X=L+S\\right\\}\\,\\,\\,\\,(3)$$\n",
    "\n",
    "Not that $S$ refers to outliers, i.e. it  represents the difference between our original data matrix $X$ and and low rank matrix $L$. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "55fad615",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4f352e62",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "env": {},
   "interrupt_mode": "signal",
   "language": "python",
   "metadata": {
    "debugger": true
   },
   "name": "python3"
  },
  "nikola": {
   "category": "",
   "date": "2023-01-14 20:28:18 UTC",
   "description": "",
   "link": "",
   "slug": "robust-and-accelerated-pca",
   "tags": "Machine Learning",
   "title": "Robust and Accelerated PCA",
   "type": "text"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
